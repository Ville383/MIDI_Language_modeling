{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from miditok import TSD\n",
    "from config import ModelParams\n",
    "\n",
    "tokenizer = TSD(params=\"models/tokenizer_trained.json\")\n",
    "\n",
    "# https://github.com/ML-GSAI/LLaDA\n",
    "def add_gumbel_noise(logits, temperature):\n",
    "    '''\n",
    "    The Gumbel max is a method for sampling categorical distributions.\n",
    "    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n",
    "    Thus, we use float64.\n",
    "    '''\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (- torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "# https://github.com/ML-GSAI/LLaDA\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    '''\n",
    "    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n",
    "    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n",
    "    the expected number of tokens transitioned at each step should be consistent.\n",
    "\n",
    "    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n",
    "    '''\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "\n",
    "    return num_transfer_tokens\n",
    "\n",
    "# https://github.com/ML-GSAI/LLaDA\n",
    "@ torch.no_grad()\n",
    "def generate(model, prompt, steps=512, gen_length=512, block_length=512, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=tokenizer['MASK_None']):\n",
    "    '''\n",
    "    Args:\n",
    "        model: Mask predictor.\n",
    "        prompt: A tensor of shape (1, L).\n",
    "        steps: Sampling steps, less than or equal to gen_length.\n",
    "        gen_length: Generated answer length.\n",
    "        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.\n",
    "        temperature: Categorical distribution sampling temperature.\n",
    "        cfg_scale: Unsupervised classifier-free guidance scale.\n",
    "        remasking: Remasking strategy. 'low_confidence' or 'random'.\n",
    "        mask_id: The toke id of [MASK] is tokenizer['MASK_None'].\n",
    "    '''\n",
    "    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(prompt.device)\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "\n",
    "    assert steps % num_blocks == 0\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    for num_block in range(num_blocks):\n",
    "        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\n",
    "        for i in range(steps):\n",
    "            mask_index = (x == mask_id)\n",
    "            if cfg_scale > 0.:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_)\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x)\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l\n",
    "\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits, dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(remasking)\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\n",
    "                transfer_index[j, select_index] = True\n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MIDI\n",
    "from pathlib import Path\n",
    "from model.mask_predictor import MaskPredictor\n",
    "from utils import MIDIDataset_sft, collate_fn_sft\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pickle_path = Path(\"sft_dataset/test/data.pkl\")\n",
    "test_dataset = MIDIDataset_sft(pickle_path)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, collate_fn=collate_fn_sft, shuffle=True)  \n",
    "\n",
    "device = torch.device('cuda')\n",
    "mask_predictor = MaskPredictor(ModelParams, tokenizer).to(device)\n",
    "checkpoint = torch.load(\"models/best_val_checkpoint_sft.pth\", map_location=device)\n",
    "mask_predictor.load_state_dict(checkpoint['model_state_dict'])\n",
    "mask_predictor.eval()\n",
    "\n",
    "gen_length = 128\n",
    "for data in test_dataloader:\n",
    "    input_ids = data['input_ids'].to(device) # [prompt + answer + padding], length=1024\n",
    "    prompt_lengths = data['prompt_lengths'].to(device)  # prompt length\n",
    "    length = data['lengths'].to(device) # [prompt + answer] length\n",
    "    max_length = length.max().item()\n",
    "    input_ids = input_ids[:, :(prompt_lengths[0] + gen_length)] # we don't know the actual answer length. Use gen_length.\n",
    "    prompt = input_ids[:, :prompt_lengths[0]]\n",
    "    answer = input_ids[:, prompt_lengths[0]:]\n",
    "\n",
    "    out = generate(mask_predictor, prompt, steps=gen_length, gen_length=gen_length, block_length=1, temperature=0., cfg_scale=0., remasking='low_confidence')\n",
    "    out = out[:, prompt.shape[1]:]\n",
    "    out_true = tokenizer([prompt.squeeze().tolist(), answer.squeeze().tolist()])\n",
    "    out_pred = tokenizer([prompt.squeeze().tolist(), out.squeeze().tolist()])\n",
    "    out_true.dump_midi(Path(\"midi_real.mid\"))\n",
    "    out_pred.dump_midi(Path(\"midi_gen.mid\"))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
